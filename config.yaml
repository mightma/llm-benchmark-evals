# LLM Evaluation Configuration

# Model settings
models:
  - model: "QuantTrio/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8"  # HuggingFace model ID or local path - used for both server startup and inference calls
    quantization: null  # "gptq"  # Not used, the vllm server will raise error if added. However, it will be automatically extracted from the model config file by the vllm server. Additionally, the example scritpt of starting the vllm server of this model, excludes this parameter, see: https://huggingface.co/QuantTrio/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8.
    gpu_memory_utilization: 0.9
    max_model_len: 262144
    tensor_parallel_size: 4
    enable_expert_parallel: true
    presence_penalty: null  # Presence penalty for VLLM server (null = use default)

# VLLM Server settings
vllm_server:
  host: "0.0.0.0"
  port: 8000
  deploy_locally: true  # Set to false to use external server
  external_host: null   # Set if using external server
  external_port: null   # Set if using external server

# Inference parameters
inference:
  temperature: 0.7
  top_p: 0.8
  top_k: 20
  max_tokens: 32768
  repetition_penalty: 1.0
  presence_penalty: null   # Presence penalty for inference (null = use default, range: -2.0 to 2.0)
  # Multiple responses settings
  max_concurrent_responses: 4  # Maximum concurrent responses when num_responses > 1
  # HTTP request retry settings
  max_retries: 3           # Number of retries for failed requests
  retry_delay: 2.0         # Initial delay between retries (seconds)
  request_timeout: 300     # Base timeout for requests (seconds)

# Benchmark settings
benchmarks:
  mmlu_pro:
    enabled: true
    data_path: "data/mmlu_pro"
    num_samples: null  # null for all samples
    subjects: null     # null for all subjects

  aime25:
    enabled: true
    data_path: "data/aime25"  # Will auto-load from HuggingFace math-ai/aime25 if local data not found
    num_samples: null

  ifeval:
    enabled: true
    data_path: "data/ifeval"  # Will auto-load from HuggingFace google/IFEval if local data not found
    num_samples: null

# Evaluation settings
evaluation:
  output_dir: "results"
  save_predictions: true
  batch_size: 1
  max_concurrent: 4
  timeout: 300  # seconds per request
  num_responses: 1  # Number of responses to generate per question (for self-consistency)

# Logging
logging:
  level: "INFO"
  file: "evaluation.log"