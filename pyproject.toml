[project]
name = "llm-benchmark-evals"
version = "0.1.0"
description = "A comprehensive Python framework for evaluating Large Language Models on standardized benchmarks including MMLU-Pro, AIME25, and IFEval using VLLM for efficient inference."
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "accelerate>=1.11.0",
    "aiohttp>=3.13.2",
    "bert-score>=0.3.13",
    "black>=25.9.0",
    "click>=8.2.1",
    "datasets>=4.3.0",
    "fastapi>=0.121.0",
    "flake8>=7.3.0",
    "httpx>=0.28.1",
    "nltk>=3.9.2",
    "numpy>=2.2.6",
    "pandas>=2.3.3",
    "pytest>=8.4.2",
    "python-dotenv>=1.2.1",
    "pyyaml>=6.0.3",
    "rich>=14.2.0",
    "rouge-score>=0.1.2",
    "scikit-learn>=1.7.2",
    "torch>=2.7.0",
    "tqdm>=4.67.1",
    "transformers>=4.57.1",
    "uvicorn>=0.38.0",
    "vllm==0.9.2",
]
